services:
  api:
    build: .
    env_file:
      - .env
    environment:
      LLM_BASE_URL: "http://llm:8000"
      LLM_MODEL: "/models/Llama-3.1-8B-Instruct-AWQ"
    ports:
      - "8000:8000"
    depends_on:
      llm:
        condition: service_started

  llm:
    image: vllm/vllm-openai:v0.15.0-cu130
    ipc: host
    gpus: all
    restart: unless-stopped

    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      NVIDIA_DISABLE_CUDA_COMPAT: "1"
      PYTORCH_ALLOC_CONF: "expandable_segments:True"

    volumes:
      - ./models:/models

    ports:
      - "8001:8000"

    command:
      - serve
      - /models/Llama-3.1-8B-Instruct-AWQ
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --quantization
      - awq_marlin
      - --max-model-len
      - "2048"
      - --gpu-memory-utilization
      - "0.90"
