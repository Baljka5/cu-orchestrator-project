version: "3.9"

services:
  api:
    build: .
    env_file:
      - .env
    ports:
      - "8000:8000"
    depends_on:
      - llm

  llm:
    image: vllm/vllm-openai:v0.15.0-cu130
    ipc: host

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    environment:
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      NVIDIA_DISABLE_CUDA_COMPAT: "1"
      PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
      LD_LIBRARY_PATH: "/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu"

    volumes:
      - ./models:/models
    ports:
      - "8001:8000"

    entrypoint: ["/bin/bash", "-lc"]
    command: |
      set -eux
      nvidia-smi
      python3 -c "import torch; print(torch.__version__, torch.cuda.is_available(), torch.cuda.device_count())"
      vllm serve /models/Llama-3.1-8B-Instruct-AWQ --host 0.0.0.0 --port 8000 --quantization awq_marlin
