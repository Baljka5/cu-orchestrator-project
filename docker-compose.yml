services:
  api:
    build: .
    env_file:
      - .env
    environment:
      LLM_BASE_URL: "http://llm:8000"
      LLM_MODEL: "/models/Llama-3.1-8B-Instruct-AWQ"
    ports:
      - "8000:8000"
    depends_on:
      llm:
        condition: service_healthy

  llm:
    image: vllm/vllm-openai:v0.15.0-cu130
    ipc: host
    gpus: all
    restart: unless-stopped

    environment:
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      NVIDIA_DISABLE_CUDA_COMPAT: "1"
      PYTORCH_ALLOC_CONF: "expandable_segments:True"
      UV_HTTP_TIMEOUT: "500"
      UV_INDEX_STRATEGY: "unsafe-best-match"
      UV_LINK_MODE: "copy"

    volumes:
      - ./models:/models:ro

    ports:
      - "8001:8000"

    entrypoint: []
    command:
      - /bin/bash
      - -lc
      - |
        set -eux
        echo "NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES}"
        nvidia-smi

        rm -f /usr/local/cuda-13.0/compat/libcuda.so* /usr/local/cuda/compat/libcuda.so* || true
        ldconfig || true

        python3 - <<'PY'
        import torch
        print("torch:", torch.__version__)
        print("cuda available:", torch.cuda.is_available())
        print("cuda device count:", torch.cuda.device_count())
        PY

        exec vllm serve /models/Llama-3.1-8B-Instruct-AWQ \
          --host 0.0.0.0 --port 8000 \
          --quantization awq_marlin \
          --max-model-len 2048 \
          --gpu-memory-utilization 0.90


    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import urllib.request; urllib.request.urlopen('http://127.0.0.1:8000/v1/models', timeout=2).read(); print('ok')\""]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 40s
